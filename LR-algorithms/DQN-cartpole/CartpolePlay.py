# -*- coding: utf-8 -*-
"""cartpole_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xsI0rwgkHUrNXT_ECqzHbMTIg8clZy7X
"""

import random
import gym
import numpy as np
from tensorflow.keras.layers import Dense, InputLayer
from collections import deque
import tensorflow as tf
from Memory import Memory
from DQNetwork import QNetwork

# パラメータの準備
NUM_EPISODES = 500 # エピソード数
MAX_STEPS = 200 # 最大ステップ数
GAMMA = 0.99 # 時間割引率
WARMUP = 10 # 無操作ステップ数

# 探索パラメータ
E_START = 1.0 # εの初期値
E_STOP = 0.01 # εの最終値
E_DECAY_RATE = 0.001 # εの減衰率

# メモリパラメータ
MEMORY_SIZE = 10000 # 経験メモリのサイズ
BATCH_SIZE = 32 # バッチサイズ

class Environment:
    def __init__(self, env, state_size, action_size):
        # 環境の作成
        self.env = env
        self.state_size = state_size
        self.action_size = action_size 

        # main-networkの作成
        self.main_qn = QNetwork(self.state_size, self.action_size)

        # target-networkの作成
        self.target_qn = QNetwork(self.state_size, self.action_size)
        # 経験メモリの作成
        self.memory = Memory(MEMORY_SIZE)
        
    def set_default_env(self):
        # 環境の初期化
        state = self.env.reset()
        state = np.reshape(state, [1, self.state_size])
        return state 

    def run(self, state):
        # エピソード数分のエピソードを繰り返す
        total_step = 0 # 総ステップ数
        success_count = 0 # 成功数
        for episode in range(1, NUM_EPISODES+1):
            step = 0 # ステップ数
            
            # target-networkの更新
            self.target_qn.model.set_weights(self.main_qn.model.get_weights())
            
            # 1エピソードのループ
            for _ in range(1, MAX_STEPS+1):
                step += 1
                total_step += 1

                # εを減らす
                epsilon = E_STOP + (E_START - E_STOP)*np.exp(-E_DECAY_RATE*total_step)
                
                # ランダムな行動を選択
                if epsilon > np.random.rand():
                    action = self.env.action_space.sample()
                # 行動価値関数で行動を選択
                else:
                    action = np.argmax(self.main_qn.model.predict(state)[0])

                # 行動に応じて状態と報酬を得る
                next_state, _, done, _ = self.env.step(action)
                next_state = np.reshape(next_state, [1, self.state_size])

                # エピソード完了時
                if done:
                    # 報酬の指定
                    if step >= 190:
                        success_count += 1
                        reward = 1
                    else:
                        success_count = 0
                        reward = 0
                    
                    # 次の状態に状態なしを代入
                    next_state = np.zeros(state.shape)
                    
                    # 経験の追加
                    if step > WARMUP:
                        self.memory.add((state, action, reward, next_state))                        
                # エピソード完了でない時
                else:
                    # 報酬の指定
                    reward = 0
                        
                    # 経験の追加
                    if step > WARMUP:
                        self.memory.add((state, action, reward, next_state))
                    
                    # 状態に次の状態を代入
                    state = next_state

                # 行動価値関数の更新
                if len(self.memory) >= BATCH_SIZE:
                    # ニューラルネットワークの入力と出力の準備
                    inputs = np.zeros((BATCH_SIZE, 4)) # 入力(状態)
                    targets = np.zeros((BATCH_SIZE, 2)) # 出力(行動ごとの価値)

                    # バッチサイズ分の経験をランダムに取得
                    minibatch = self.memory.sample(BATCH_SIZE)
                    
                    # ニューラルネットワークの入力と出力の生成
                    for i, (state_b, action_b, reward_b, next_state_b) in enumerate(minibatch):
                        
                        # 入力に状態を指定
                        inputs[i] = state_b
                        
                        # 採った行動の価値を計算
                        if not (next_state_b == np.zeros(state_b.shape)).all(axis=1):
                            target = reward_b + GAMMA * np.amax(self.target_qn.model.predict(next_state_b)[0])
                        else:
                            target = reward_b

                        # 出力に行動ごとの価値を指定
                        targets[i] = self.main_qn.model.predict(state_b)
                        targets[i][action_b] = target # 採った行動の価値

                    # 行動価値関数の更新
                    self.main_qn.model.fit(inputs, targets, epochs=1, verbose=0)
                
                # エピソード完了時
                if done:
                    # エピソードループを抜ける
                    break
                  
            # エピソード完了時のログ表示
            print('エピソード: {}, ステップ数: {}, epsilon: {:.4f}'.format(episode, step, epsilon))

            # 5回連続成功で学習終了
            if success_count >= 5:
                break

            # 環境のリセット
            state = self.set_default_env()

env = gym.make('CartPole-v0')
state_size = env.observation_space.shape[0] # 行動数
action_size = env.action_space.n # 状態数
cartpole_env = Environment(env, state_size, action_size)

state = cartpole_env.set_default_env()
cartpole_env.run(state)

